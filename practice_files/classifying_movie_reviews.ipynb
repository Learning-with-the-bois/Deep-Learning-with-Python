{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying movie reviews\n",
    "**A binary classification example**\n",
    "\n",
    "Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem.\n",
    "In this example, movie reviews will be classified as positive or negative, based on the text content of the reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset\n",
    "You’ll work with the [IMDB dataset](https://www.imdb.com/): a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels),(test_data,test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `num_words=10000` means you'll only keep the top 10,000 most frequently ocurring words in the training data. Rare words will be discarded.\n",
    "\n",
    "* `train_data` an `test_data` are lists of reviews. Each review is a list of word indices ranked by frequency.\n",
    "\n",
    "* `train_labels` and `test_labels` are lists of 0s and 1s,where 0 stands for *negative* and 1 for *positive*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "You have to turn the lists into tensors. There are 2 ways to do that:\n",
    "* Pad the lists so that they all have the same length, turn them into an integer tensor of shape `(samples, word_indices)`, and use as the first layer in the network a layer capable of handling such integer tensors. (The *Embedding* layer).\n",
    "* One-hot encode the lists and turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence `[3,5]` into a 10,000-dimentional vector that would be all 0s except for `[3,5]`. The use a *Dense* layer as first layer, capable of handling floating-point vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence]=1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model definition\n",
    "There are two key architecture decisions to be made about such a stack of Dense layers:\n",
    "* How many layers to use\n",
    "* How many hidden units to choose for each layer\n",
    "\n",
    "\n",
    "**Hidden unit:** Dimension in the representation space of the layer.\n",
    "* *Intuitively ->* \"How much freedom you're allowing the network to have when learning internal representations.\"\n",
    "\n",
    "Having more hidden units allows the network to learn more complex representations, but it makes the netwoek mor computotionally expensive and may lead to learning unwanted patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation=\"relu\", input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "Finally, you need to choose a **loss function** and an **optimizer**.\n",
    "In this case, *binary_crossentropy* is the best option as **loss function**, because we are dealing with output probabilities. **Crossentropy** is a quantity from the field of Information THeory that measures the distance between probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Optmizer, losses and metrics\n",
    "You can pass and optimizer class instance as the `oṕtimizer` argument, and  pass function objects as the `loss` or `metrics` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "customOptimizer = optimizer=optimizers.RMSprop(lr=0.001)\n",
    "customLoss = losses.binary_crossentropy\n",
    "customMetric = [metrics.binary_accuracy]\n",
    "\n",
    "model.compile(optimizer=customOptimizer,\n",
    "loss=customLoss,\n",
    "metrics=customMetric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the approach\n",
    "### Set the validation set\n",
    "In order to test the model you need to create a validation set by setting apart 10,000 samples from the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_test = x_train[:10000]\n",
    "x_train_values = x_train[10000:]\n",
    "\n",
    "y_training_test = y_train[:10000]\n",
    "y_train_values = y_train[10000:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We'll be training the model for 20 epochs, in mini-batches of 512 samples. At the same time, we'll monitor loss and accuracy on the 10,000 samples setted appart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_values,\n",
    "                    y_train_values\n",
    "                    epochs=20,\n",
    "                    batch_size=512\n",
    "                    validation_data=(x_training_test,y_training_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
